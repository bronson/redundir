#!/usr/bin/env python3
"""
redundir - Find directories containing duplicate files

This utility recursively walks a directory tree looking for directories
containing duplicate files. It prints a list of directories found that
contain duplicate files, along with their redundancy score, sorted by
redundancy score (most redundant first).

The redundancy score for each directory is:
    number of duplicate files / number of files in the directory
"""

import argparse
import hashlib
import os
import sys
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path


def hash_file(filepath: Path, algorithm: str = 'blake2b', chunk_size: int = 8388608) -> str | None:
    """
    Compute hash of a file.

    Args:
        filepath: Path to the file to hash
        algorithm: Hash algorithm to use (default: blake2b)
        chunk_size: Size of chunks to read (default 8MB)

    Returns:
        Hex digest of the file's hash, or None if file couldn't be read
    """
    hasher = hashlib.new(algorithm)
    try:
        with open(filepath, 'rb') as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    except (IOError, OSError, PermissionError):
        return None


def _hash_file_worker(args):
    """Worker function for multiprocessing hash computation."""
    filepath, algorithm = args
    return filepath, hash_file(filepath, algorithm)


def find_duplicates(root_path: Path, algorithm: str = 'blake2b', jobs: int = 4, quiet: bool = False) -> dict[Path, tuple[int, int]]:
    """
    Walk directory tree and find directories with duplicate files.

    A file is considered a duplicate if another file with the same content
    (same hash) exists anywhere in the scanned tree.

    Args:
        root_path: Root directory to scan
        algorithm: Hash algorithm to use
        jobs: Number of parallel jobs for hashing
        quiet: If True, suppress progress information

    Returns:
        Dict mapping directory path to (duplicate_count, total_count)
    """
    # First pass: collect all files and group by size
    size_groups: dict[int, list[Path]] = defaultdict(list)
    dir_all_files: dict[Path, list[Path]] = defaultdict(list)  # dir -> list of file paths

    file_count = 0

    if not quiet:
        print(f"  Collecting files...", file=sys.stderr)

    for dirpath, dirnames, filenames in os.walk(root_path):
        dir_path = Path(dirpath)

        for filename in filenames:
            file_path = dir_path / filename

            # Skip symlinks to avoid counting the same file multiple times
            if file_path.is_symlink():
                continue

            if file_path.is_file():
                try:
                    file_size = file_path.stat().st_size
                    size_groups[file_size].append(file_path)
                    dir_all_files[dir_path].append(file_path)
                    file_count += 1
                except (IOError, OSError):
                    continue

    if not quiet:
        print(f"  Found {file_count} files", file=sys.stderr)

    # Only hash files that have at least one other file with the same size
    files_to_hash = []
    for size, paths in size_groups.items():
        if len(paths) > 1:
            files_to_hash.extend(paths)

    if not quiet:
        print(f"  Need to hash {len(files_to_hash)} files (skipped {file_count - len(files_to_hash)} unique sizes)", file=sys.stderr)

    # Hash files in parallel
    file_hashes: dict[str, list[Path]] = defaultdict(list)

    if files_to_hash:
        hash_args = [(fpath, algorithm) for fpath in files_to_hash]

        if jobs > 1:
            with ProcessPoolExecutor(max_workers=jobs) as executor:
                for i, (filepath, file_hash) in enumerate(executor.map(_hash_file_worker, hash_args), 1):
                    if file_hash:
                        file_hashes[file_hash].append(filepath)

                    if not quiet and i % 1000 == 0:
                        print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)
        else:
            # Single-threaded mode
            for i, (filepath, algorithm_arg) in enumerate(hash_args, 1):
                file_hash = hash_file(filepath, algorithm_arg)
                if file_hash:
                    file_hashes[file_hash].append(filepath)

                if not quiet and i % 1000 == 0:
                    print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)

    if not quiet:
        print(f"  Hashed {len(files_to_hash)} total files", file=sys.stderr)

    # Find hashes that appear more than once (duplicates)
    duplicate_hashes = {h for h, paths in file_hashes.items() if len(paths) > 1}

    if not quiet:
        print(f"  Found {len(duplicate_hashes)} unique duplicate content hashes", file=sys.stderr)

    # Build a map of file path to hash for duplicate files
    file_to_hash = {}
    for hash_val, paths in file_hashes.items():
        if hash_val in duplicate_hashes:
            for path in paths:
                file_to_hash[path] = hash_val

    # Calculate redundancy for each directory
    dir_stats: dict[Path, tuple[int, int]] = {}

    for dir_path, file_paths in dir_all_files.items():
        total = len(file_paths)
        duplicates = sum(1 for fpath in file_paths if fpath in file_to_hash)
        if duplicates > 0:
            dir_stats[dir_path] = (duplicates, total)

    return dir_stats


def calculate_redundancy_score(duplicates: int, total: int) -> float:
    """Calculate redundancy score as a ratio of duplicates to total files."""
    return duplicates / total if total > 0 else 0.0


def main():
    parser = argparse.ArgumentParser(
        prog='redundir',
        description='Find directories containing duplicate files and report redundancy scores.',
        epilog='The redundancy score is: (duplicate files in dir) / (total files in dir)'
    )
    parser.add_argument(
        'directory',
        nargs='?',
        default='.',
        help='Directory to scan (default: current directory)'
    )
    parser.add_argument(
        '-a', '--algorithm',
        default='blake2b',
        choices=['md5', 'sha1', 'sha256', 'blake2b', 'blake2s'],
        help='Hash algorithm to use (default: blake2b)'
    )
    parser.add_argument(
        '-j', '--jobs',
        type=int,
        default=4,
        metavar='N',
        help='Number of parallel hashing jobs (default: 4, use 1 to disable)'
    )
    parser.add_argument(
        '-q', '--quiet',
        action='store_true',
        help='Suppress progress messages'
    )

    args = parser.parse_args()

    root = Path(args.directory).resolve()

    if not root.exists():
        print(f"Error: '{args.directory}' does not exist", file=sys.stderr)
        sys.exit(1)

    if not root.is_dir():
        print(f"Error: '{args.directory}' is not a directory", file=sys.stderr)
        sys.exit(1)

    if not args.quiet:
        print(f"Scanning {root}...", file=sys.stderr)

    dir_stats = find_duplicates(root, algorithm=args.algorithm, jobs=args.jobs, quiet=args.quiet)

    if not dir_stats:
        if not args.quiet:
            print("No duplicate files found.", file=sys.stderr)
        sys.exit(0)

    # Sort by redundancy score (descending), then by path for stable ordering
    sorted_dirs = sorted(
        dir_stats.items(),
        key=lambda x: (-calculate_redundancy_score(x[1][0], x[1][1]), str(x[0]))
    )

    # Print results with relative paths in aligned columns
    # First pass: prepare all rows and find column widths
    rows = []
    max_count_width = 0

    for dir_path, (duplicates, total) in sorted_dirs:
        score = calculate_redundancy_score(duplicates, total)
        # Convert to relative path from root
        try:
            relative_path = dir_path.relative_to(root)
            display_path = relative_path if str(relative_path) != '.' else '.'
        except ValueError:
            # If path is not relative to root, use absolute
            display_path = dir_path

        count_str = f"{duplicates}/{total}"
        max_count_width = max(max_count_width, len(count_str))
        rows.append((score, count_str, display_path))

    # Second pass: print with aligned columns
    for score, count_str, display_path in rows:
        # Format percentage with proper alignment (right-align to 7 chars including %)
        percent_str = f"{score:7.2%}"
        print(f"{percent_str}  {count_str:>{max_count_width}}  {display_path}")

    if not args.quiet:
        print(f"\nFound {len(dir_stats)} directories with duplicate files.", file=sys.stderr)


if __name__ == '__main__':
    main()
