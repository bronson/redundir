#!/usr/bin/env python3
"""
redundir - Find directories containing duplicate files

This utility recursively walks a directory tree looking for directories
containing duplicate files. It prints a list of directories found that
contain duplicate files, along with their redundancy score, sorted by
redundancy score (most redundant first).

The redundancy score for each directory is:
    number of duplicate files / number of files in the directory
"""

import argparse
import hashlib
import os
import sys
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path


def hash_file(filepath: Path, algorithm: str = 'blake2b', chunk_size: int = 8388608) -> str | None:
    """
    Compute hash of a file.

    Args:
        filepath: Path to the file to hash
        algorithm: Hash algorithm to use (default: blake2b)
        chunk_size: Size of chunks to read (default 8MB)

    Returns:
        Hex digest of the file's hash, or None if file couldn't be read
    """
    hasher = hashlib.new(algorithm)
    try:
        with open(filepath, 'rb') as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    except (IOError, OSError, PermissionError):
        return None


def _hash_file_worker(args):
    """Worker function for multiprocessing hash computation."""
    filepath, algorithm = args
    return filepath, hash_file(filepath, algorithm)


def find_duplicates(root_paths, algorithm: str = 'blake2b', jobs: int = 4, quiet: bool = False):
    """
    Walk directory trees and find directories with duplicate files.

    A file is considered a duplicate if another file with the same content
    (same hash) exists anywhere in the scanned trees.

    Args:
        root_paths: Root directory or list of root directories to scan
        algorithm: Hash algorithm to use
        jobs: Number of parallel jobs for hashing
        quiet: If True, suppress progress information

    Returns:
        Tuple of (dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files)
    """
    # Normalize to list
    if isinstance(root_paths, Path):
        root_paths = [root_paths]

    # First pass: collect all files and group by size
    size_groups: dict[int, list[Path]] = defaultdict(list)
    dir_all_files: dict[Path, list[Path]] = defaultdict(list)  # dir -> list of file paths

    file_count = 0

    if not quiet:
        print(f"  Collecting files...", file=sys.stderr)

    for root_path in root_paths:
        for dirpath, dirnames, filenames in os.walk(root_path):
            dir_path = Path(dirpath)

            for filename in filenames:
                file_path = dir_path / filename

                # Skip symlinks to avoid counting the same file multiple times
                if file_path.is_symlink():
                    continue

                if file_path.is_file():
                    try:
                        file_size = file_path.stat().st_size
                        size_groups[file_size].append(file_path)
                        dir_all_files[dir_path].append(file_path)
                        file_count += 1
                    except (IOError, OSError):
                        continue

    if not quiet:
        print(f"  Found {file_count} total files", file=sys.stderr)

    # Only hash files that have at least one other file with the same size
    files_to_hash = []
    for size, paths in size_groups.items():
        if len(paths) > 1:
            files_to_hash.extend(paths)

    if not quiet:
        print(f"  Hashing {len(files_to_hash)} files, skipping {file_count - len(files_to_hash)} files with unique sizes", file=sys.stderr)

    # Hash files in parallel
    file_hashes: dict[str, list[Path]] = defaultdict(list)

    if files_to_hash:
        hash_args = [(fpath, algorithm) for fpath in files_to_hash]

        if jobs > 1:
            with ProcessPoolExecutor(max_workers=jobs) as executor:
                for i, (filepath, file_hash) in enumerate(executor.map(_hash_file_worker, hash_args), 1):
                    if file_hash:
                        file_hashes[file_hash].append(filepath)

                    if not quiet and i % 1000 == 0:
                        print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)
        else:
            # Single-threaded mode
            for i, (filepath, algorithm_arg) in enumerate(hash_args, 1):
                file_hash = hash_file(filepath, algorithm_arg)
                if file_hash:
                    file_hashes[file_hash].append(filepath)

                if not quiet and i % 1000 == 0:
                    print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)

    if not quiet:
        print(f"  Hashed {len(files_to_hash)} files", file=sys.stderr)

    # Find hashes that appear more than once (duplicates)
    duplicate_hashes = {h for h, paths in file_hashes.items() if len(paths) > 1}



    # Build a map of file path to hash for duplicate files
    file_to_hash = {}
    for hash_val, paths in file_hashes.items():
        if hash_val in duplicate_hashes:
            for path in paths:
                file_to_hash[path] = hash_val

    # Calculate redundancy for each directory
    dir_stats: dict[Path, tuple[int, int]] = {}
    total_files = 0
    total_duplicates = 0

    for dir_path, file_paths in dir_all_files.items():
        total = len(file_paths)
        duplicates = sum(1 for fpath in file_paths if fpath in file_to_hash)
        total_files += total
        total_duplicates += duplicates
        if duplicates > 0:
            dir_stats[dir_path] = (duplicates, total)

    # Build hash to directories map for verbose mode
    hash_to_dirs: dict[str, set[Path]] = defaultdict(set)
    for hash_val, paths in file_hashes.items():
        if hash_val in duplicate_hashes:
            for path in paths:
                hash_to_dirs[hash_val].add(path.parent)

    return dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files


def calculate_redundancy_score(duplicates: int, total: int) -> float:
    """Calculate redundancy score as a ratio of duplicates to total files."""
    return duplicates / total if total > 0 else 0.0


def main():
    parser = argparse.ArgumentParser(
        prog='redundir',
        description='Find directories containing duplicate files and report redundancy scores.',
        epilog='The redundancy score is: (duplicate files in dir) / (total files in dir)'
    )
    parser.add_argument(
        'directories',
        nargs='*',
        default=['.'],
        metavar='directory',
        help='Directories to scan (default: current directory)'
    )
    parser.add_argument(
        '-a', '--algorithm',
        default='blake2b',
        choices=['md5', 'sha1', 'sha256', 'blake2b', 'blake2s'],
        help='Hash algorithm to use (default: blake2b)'
    )
    parser.add_argument(
        '-j', '--jobs',
        type=int,
        default=4,
        metavar='N',
        help='Number of parallel hashing jobs (default: 4, use 1 to disable)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Show related directories for each result'
    )
    parser.add_argument(
        '-q', '--quiet',
        action='store_true',
        help='Suppress progress messages'
    )

    args = parser.parse_args()

    # Handle both single directory (backward compat) and multiple directories
    roots = []
    for directory in args.directories:
        root = Path(directory).resolve()
        if not root.exists():
            print(f"Error: '{directory}' does not exist", file=sys.stderr)
            sys.exit(1)
        if not root.is_dir():
            print(f"Error: '{directory}' is not a directory", file=sys.stderr)
            sys.exit(1)
        roots.append(root)

    if not args.quiet:
        if len(roots) == 1:
            print(f"Scanning {roots[0]}...", file=sys.stderr)
        else:
            print(f"Scanning {len(roots)} directories...", file=sys.stderr)
            for root in roots:
                print(f"  {root}", file=sys.stderr)

    # Scan all directories together (find_duplicates handles multiple roots)
    dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files = find_duplicates(roots, algorithm=args.algorithm, jobs=args.jobs, quiet=args.quiet)

    # Print summary before results
    if not args.quiet:
        if not dir_stats:
            print("No duplicate files found.", file=sys.stderr)
        else:
            overall_redundancy = calculate_redundancy_score(total_duplicates, total_files)
            print(f"  Found {total_duplicates} duplicate files in {len(dir_stats)} directories (overall redundancy: {overall_redundancy:.2%})", file=sys.stderr)
            print("", file=sys.stderr)

    if not dir_stats:
        sys.exit(0)

    # Sort by redundancy score (descending), then by path for stable ordering
    sorted_dirs = sorted(
        dir_stats.items(),
        key=lambda x: (-calculate_redundancy_score(x[1][0], x[1][1]), str(x[0]))
    )

    # Print results with relative paths in aligned columns
    # First pass: prepare all rows and find column widths
    rows = []
    max_count_width = 0

    for dir_path, (duplicates, total) in sorted_dirs:
        score = calculate_redundancy_score(duplicates, total)
        # Convert to relative path from one of the roots
        display_path = None
        for root in roots:
            try:
                relative_path = dir_path.relative_to(root)
                display_path = str(root.name / relative_path) if str(relative_path) != '.' else str(root.name)
                break
            except ValueError:
                continue
        if display_path is None:
            # If path is not relative to any root, use absolute
            display_path = dir_path

        count_str = f"{duplicates}/{total}"
        max_count_width = max(max_count_width, len(count_str))
        rows.append((score, count_str, display_path))

    # Second pass: print with aligned columns
    for score, count_str, display_path in rows:
        # Format percentage with proper alignment (right-align to 7 chars including %)
        percent_str = f"{score:7.2%}"
        print(f"{percent_str}  {count_str:>{max_count_width}}  {display_path}")

        # Verbose mode: show related directories
        if args.verbose:
            # Find the actual directory path from display_path
            dir_path = None
            for dp, _ in sorted_dirs:
                try:
                    rel = dp.relative_to(root)
                    if str(rel if str(rel) != '.' else '.') == str(display_path):
                        dir_path = dp
                        break
                except ValueError:
                    if dp == display_path:
                        dir_path = dp
                        break

            if dir_path:
                # Find all hashes in this directory
                dir_hashes = set()
                for fpath in dir_all_files.get(dir_path, []):
                    if fpath in file_to_hash:
                        dir_hashes.add(file_to_hash[fpath])

                # Find related directories (directories that share at least one hash with this dir)
                related_dirs = set()
                for h in dir_hashes:
                    for d in hash_to_dirs[h]:
                        if d != dir_path:
                            related_dirs.add(d)

                # Calculate hypothetical redundancy for each related directory
                related_info = []
                for related_dir in related_dirs:
                    related_files = dir_all_files.get(related_dir, [])
                    total_related = len(related_files)

                    # Count duplicates in related_dir if dir_path didn't exist
                    hypothetical_duplicates = 0
                    for fpath in related_files:
                        if fpath in file_to_hash:
                            h = file_to_hash[fpath]
                            # Count how many OTHER directories (excluding dir_path and related_dir) have this hash
                            other_dirs = hash_to_dirs[h] - {dir_path, related_dir}
                            # If there are other directories with this hash, it's still a duplicate
                            if len(other_dirs) > 0:
                                hypothetical_duplicates += 1

                    hyp_score = calculate_redundancy_score(hypothetical_duplicates, total_related)

                    # Convert to relative path
                    try:
                        rel_path = related_dir.relative_to(root)
                        rel_display = rel_path if str(rel_path) != '.' else '.'
                    except ValueError:
                        rel_display = related_dir

                    related_info.append((hyp_score, hypothetical_duplicates, total_related, rel_display))

                # Sort by hypothetical redundancy score (descending)
                related_info.sort(key=lambda x: (-x[0], str(x[3])))

                # Print related directories
                for hyp_score, hyp_dup, hyp_total, rel_display in related_info:
                    hyp_percent = f"{hyp_score:7.2%}"
                    hyp_count = f"{hyp_dup}/{hyp_total}"
                    print(f"    {hyp_percent}  {hyp_count:>{max_count_width}}  {rel_display}")


if __name__ == '__main__':
    main()
