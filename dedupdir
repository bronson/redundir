#!/usr/bin/env python3
"""
dedupdir - Find directories containing duplicate files

This utility recursively walks a directory tree looking for directories
containing duplicate files. It prints a list of directories found that
contain duplicate files, along with their redundancy score, sorted by
redundancy score (most redundant first).

The redundancy score for each directory is:
    number of duplicate files / number of files in the directory
"""

import argparse
import hashlib
import json
import os
import sys
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path

CACHE_FILE_NAME = '~dedupdir-cache.json'
CACHE_VERSION = 1


def hash_file(filepath: Path, algorithm: str = 'blake2b', chunk_size: int = 8388608) -> str | None:
    """
    Compute hash of a file.

    Args:
        filepath: Path to the file to hash
        algorithm: Hash algorithm to use (default: blake2b)
        chunk_size: Size of chunks to read (default 8MB)

    Returns:
        Hex digest of the file's hash, or None if file couldn't be read
    """
    hasher = hashlib.new(algorithm)
    try:
        with open(filepath, 'rb') as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    except (IOError, OSError, PermissionError):
        return None


def _hash_file_worker(args):
    """Worker function for multiprocessing hash computation."""
    filepath, algorithm = args
    return filepath, hash_file(filepath, algorithm)


def _verify_hash_worker(args):
    """Worker function for verifying a cached hash is still correct."""
    filepath, expected_hash, algorithm = args
    actual_hash = hash_file(filepath, algorithm)
    if actual_hash is None:
        return filepath, 'unreadable', expected_hash, None
    if actual_hash != expected_hash:
        return filepath, 'mismatch', expected_hash, actual_hash
    return filepath, 'ok', expected_hash, actual_hash


def get_cache_file(root_path):
    """Get the cache file path for a given root."""
    return Path(root_path) / CACHE_FILE_NAME


def load_cache(root_path, algorithm):
    """
    Load cache for a root directory.

    Returns:
        dict mapping relative file paths to {'hash': str, 'mtime_ns': int, 'size': int}
        or None if cache doesn't exist or is invalid
    """
    cache_file = get_cache_file(root_path)
    if not cache_file.exists():
        return None

    try:
        with open(cache_file, 'r') as f:
            data = json.load(f)

        # Validate cache version and algorithm
        if data.get('version') != CACHE_VERSION:
            return None
        if data.get('algorithm') != algorithm:
            return None

        return data.get('files', {})
    except (json.JSONDecodeError, IOError, OSError):
        return None


def save_cache(root_path, algorithm, file_cache):
    """
    Save cache for a root directory.

    Args:
        root_path: Root directory path
        algorithm: Hash algorithm used
        file_cache: dict mapping relative file paths to {'hash': str, 'mtime_ns': int, 'size': int}
    """
    cache_file = get_cache_file(root_path)

    temp_file = cache_file.with_suffix('.tmp')
    try:
        data = {
            'version': CACHE_VERSION,
            'algorithm': algorithm,
            'files': file_cache
        }

        # Write atomically using temp file
        with open(temp_file, 'w') as f:
            json.dump(data, f)
        temp_file.replace(cache_file)
    except (IOError, OSError, PermissionError):
        # Read-only filesystem or permission denied - silently skip caching
        # Clean up temp file if it was created
        try:
            if temp_file.exists():
                temp_file.unlink()
        except (IOError, OSError, PermissionError):
            pass


def validate_cache_mtimes(root_path, cache, quiet=False):
    """
    Quickly validate that cached mtimes match current file mtimes.

    Returns:
        Tuple of (valid_entries, invalid_entries, missing_entries, new_files)
        - valid_entries: dict of cache entries where mtime_ns matches
        - invalid_entries: list of relative paths where mtime_ns changed
        - missing_entries: list of relative paths that no longer exist
        - new_files: list of (absolute_path, size) for files not in cache
    """
    valid_entries = {}
    invalid_entries = []
    missing_entries = []
    new_files = []

    root_path = Path(root_path)

    # Check existing cache entries
    for rel_path, entry in cache.items():
        abs_path = root_path / rel_path
        try:
            if abs_path.is_symlink():
                # Skip symlinks - treat as missing
                missing_entries.append(rel_path)
                continue
            stat = abs_path.stat()
            # Compare mtime_ns and size (exact integer comparison)
            if stat.st_mtime_ns == entry['mtime_ns'] and stat.st_size == entry['size']:
                valid_entries[rel_path] = entry
            else:
                invalid_entries.append(rel_path)
        except (IOError, OSError):
            missing_entries.append(rel_path)

    # Find new files not in cache
    cached_rel_paths = set(cache.keys())
    for dirpath, dirnames, filenames in os.walk(root_path):
        dir_path = Path(dirpath)

        # Skip trash directories
        dirnames[:] = [d for d in dirnames if d != '~dedupdir-trash']

        for filename in filenames:
            # Skip the cache file
            if filename == CACHE_FILE_NAME:
                continue

            file_path = dir_path / filename

            if file_path.is_symlink():
                continue

            try:
                rel_path = str(file_path.relative_to(root_path))
                if rel_path not in cached_rel_paths:
                    stat = file_path.stat()
                    new_files.append((file_path, stat.st_size))
            except (IOError, OSError, ValueError):
                continue

    return valid_entries, invalid_entries, missing_entries, new_files


def collect_files_with_cache(root_paths, algorithm, quiet=False):
    """
    Collect files and use cache where possible.

    Returns:
        Tuple of (size_groups, dir_all_files, cached_hashes, files_to_hash, cache_data_per_root)
        - size_groups: dict mapping size to list of file paths
        - dir_all_files: dict mapping dir path to list of file paths
        - cached_hashes: dict mapping file path to cached hash
        - files_to_hash: list of file paths that need hashing
        - cache_data_per_root: dict mapping root to cache dict for later saving
    """
    if isinstance(root_paths, Path):
        root_paths = [root_paths]

    size_groups = defaultdict(list)
    dir_all_files = defaultdict(list)
    cached_hashes = {}  # file_path -> hash
    files_needing_hash = []  # (file_path, size)
    cache_data_per_root = {}  # root -> {rel_path -> {hash, mtime, size}}

    total_cached = 0
    total_new = 0

    for root_path in root_paths:
        root_path = Path(root_path)
        cache = load_cache(root_path, algorithm)

        if cache:
            if not quiet:
                print(f"  Loading cache for {root_path.name}...", file=sys.stderr)

            valid, invalid, missing, new_files = validate_cache_mtimes(root_path, cache, quiet)

            if not quiet:
                print(f"    {len(valid)} cached, {len(invalid)} changed, {len(missing)} removed, {len(new_files)} new", file=sys.stderr)

            # Use valid cache entries
            for rel_path, entry in valid.items():
                abs_path = root_path / rel_path
                dir_path = abs_path.parent
                size = entry['size']

                size_groups[size].append(abs_path)
                dir_all_files[dir_path].append(abs_path)
                cached_hashes[abs_path] = entry['hash']
                total_cached += 1

            # Queue changed files for rehashing
            for rel_path in invalid:
                abs_path = root_path / rel_path
                try:
                    stat = abs_path.stat()
                    dir_path = abs_path.parent
                    size_groups[stat.st_size].append(abs_path)
                    dir_all_files[dir_path].append(abs_path)
                    files_needing_hash.append((abs_path, stat.st_size, stat.st_mtime_ns))
                    total_new += 1
                except (IOError, OSError):
                    pass

            # Queue new files for hashing
            for abs_path, size in new_files:
                dir_path = abs_path.parent
                size_groups[size].append(abs_path)
                dir_all_files[dir_path].append(abs_path)
                try:
                    mtime_ns = abs_path.stat().st_mtime_ns
                    files_needing_hash.append((abs_path, size, mtime_ns))
                    total_new += 1
                except (IOError, OSError):
                    pass

            # Start with valid entries for this root's cache
            cache_data_per_root[root_path] = {rel_path: entry for rel_path, entry in valid.items()}
        else:
            # No cache - collect all files
            if not quiet:
                print(f"  Collecting files from {root_path.name} (no cache)...", file=sys.stderr)

            cache_data_per_root[root_path] = {}

            for dirpath, dirnames, filenames in os.walk(root_path):
                dir_path = Path(dirpath)

                # Skip trash directories
                dirnames[:] = [d for d in dirnames if d != '~dedupdir-trash']

                for filename in filenames:
                    # Skip the cache file
                    if filename == CACHE_FILE_NAME:
                        continue

                    file_path = dir_path / filename

                    if file_path.is_symlink():
                        continue

                    if file_path.is_file():
                        try:
                            stat = file_path.stat()
                            size_groups[stat.st_size].append(file_path)
                            dir_all_files[dir_path].append(file_path)
                            files_needing_hash.append((file_path, stat.st_size, stat.st_mtime_ns))
                            total_new += 1
                        except (IOError, OSError):
                            continue

    if not quiet and total_cached > 0:
        print(f"  {total_cached} files from cache, {total_new} files to process", file=sys.stderr)

    return size_groups, dir_all_files, cached_hashes, files_needing_hash, cache_data_per_root


def get_files_to_verify(root_paths, cached_hashes, algorithm):
    """
    Get list of cached files that should be verified in the background.

    Returns:
        List of (filepath, expected_hash, algorithm) tuples
    """
    files_to_verify = []
    for filepath, expected_hash in cached_hashes.items():
        files_to_verify.append((filepath, expected_hash, algorithm))
    return files_to_verify


def find_duplicates(root_paths, algorithm: str = 'blake2b', jobs: int = 4, quiet: bool = False, use_cache: bool = True):
    """
    Walk directory trees and find directories with duplicate files.

    A file is considered a duplicate if another file with the same content
    (same hash) exists anywhere in the scanned trees.

    Args:
        root_paths: Root directory or list of root directories to scan
        algorithm: Hash algorithm to use
        jobs: Number of parallel jobs for hashing
        quiet: If True, suppress progress information
        use_cache: If True, use cached hashes where available

    Returns:
        Tuple of (dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files, cached_hashes)
        The cached_hashes dict maps file paths to their hashes for files that were loaded from cache.
    """
    # Normalize to list
    if isinstance(root_paths, Path):
        root_paths = [root_paths]

    # Collect files, using cache if available
    if use_cache:
        size_groups, dir_all_files, cached_hashes, files_needing_hash, cache_data_per_root = \
            collect_files_with_cache(root_paths, algorithm, quiet)
        file_count = sum(len(files) for files in dir_all_files.values())
    else:
        # Original behavior - no cache
        size_groups = defaultdict(list)
        dir_all_files = defaultdict(list)
        cached_hashes = {}
        files_needing_hash = []
        cache_data_per_root = {Path(rp): {} for rp in root_paths}
        file_count = 0

        if not quiet:
            print(f"  Collecting files...", file=sys.stderr)

        for root_path in root_paths:
            for dirpath, dirnames, filenames in os.walk(root_path):
                dir_path = Path(dirpath)

                # Skip trash directories
                dirnames[:] = [d for d in dirnames if d != '~dedupdir-trash']

                for filename in filenames:
                    # Skip the cache file
                    if filename == CACHE_FILE_NAME:
                        continue

                    file_path = dir_path / filename

                    if file_path.is_symlink():
                        continue

                    if file_path.is_file():
                        try:
                            stat = file_path.stat()
                            size_groups[stat.st_size].append(file_path)
                            dir_all_files[dir_path].append(file_path)
                            files_needing_hash.append((file_path, stat.st_size, stat.st_mtime_ns))
                            file_count += 1
                        except (IOError, OSError):
                            continue

        if not quiet:
            print(f"  Found {file_count} total files", file=sys.stderr)

    # Determine which files need hashing (same-size optimization)
    # For files from cache, we already have their hashes
    # For new files, only hash if there's another file with same size
    files_to_hash = []
    for fpath, size, mtime in files_needing_hash:
        # Check if there's another file with this size (could be cached or new)
        if len(size_groups[size]) > 1:
            files_to_hash.append((fpath, size, mtime))

    if not quiet and files_to_hash:
        print(f"  Hashing {len(files_to_hash)} files...", file=sys.stderr)

    # Hash files in parallel
    file_hashes: dict[str, list[Path]] = defaultdict(list)

    # First, add all cached hashes
    for fpath, fhash in cached_hashes.items():
        file_hashes[fhash].append(fpath)

    # Then hash new files
    if files_to_hash:
        hash_args = [(fpath, algorithm) for fpath, size, mtime in files_to_hash]

        if jobs > 1:
            with ProcessPoolExecutor(max_workers=jobs) as executor:
                for i, (filepath, file_hash) in enumerate(executor.map(_hash_file_worker, hash_args), 1):
                    if file_hash:
                        file_hashes[file_hash].append(filepath)

                        # Update cache for this file
                        for root_path in cache_data_per_root:
                            try:
                                rel_path = str(filepath.relative_to(root_path))
                                # Find the matching entry in files_to_hash to get mtime_ns
                                for fpath, size, mtime_ns in files_to_hash:
                                    if fpath == filepath:
                                        cache_data_per_root[root_path][rel_path] = {
                                            'hash': file_hash,
                                            'mtime_ns': mtime_ns,
                                            'size': size
                                        }
                                        break
                                break
                            except ValueError:
                                continue

                    if not quiet and i % 1000 == 0:
                        print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)
        else:
            # Single-threaded mode
            for i, (fpath, size, mtime_ns) in enumerate(files_to_hash, 1):
                file_hash = hash_file(fpath, algorithm)
                if file_hash:
                    file_hashes[file_hash].append(fpath)

                    # Update cache for this file
                    for root_path in cache_data_per_root:
                        try:
                            rel_path = str(fpath.relative_to(root_path))
                            cache_data_per_root[root_path][rel_path] = {
                                'hash': file_hash,
                                'mtime_ns': mtime_ns,
                                'size': size
                            }
                            break
                        except ValueError:
                            continue

                if not quiet and i % 1000 == 0:
                    print(f"  Hashed {i}/{len(files_to_hash)} files...", file=sys.stderr)

    if not quiet and files_to_hash:
        print(f"  Hashed {len(files_to_hash)} files", file=sys.stderr)

    # Save updated caches
    if use_cache:
        for root_path, cache_data in cache_data_per_root.items():
            if cache_data:  # Only save if there's data
                save_cache(root_path, algorithm, cache_data)

    # Find hashes that appear more than once (duplicates)
    duplicate_hashes = {h for h, paths in file_hashes.items() if len(paths) > 1}

    # Build a map of file path to hash for duplicate files
    file_to_hash = {}
    for hash_val, paths in file_hashes.items():
        if hash_val in duplicate_hashes:
            for path in paths:
                file_to_hash[path] = hash_val

    # Calculate redundancy for each directory
    dir_stats: dict[Path, tuple[int, int]] = {}
    total_files = 0
    total_duplicates = 0

    for dir_path, file_paths in dir_all_files.items():
        total = len(file_paths)
        duplicates = sum(1 for fpath in file_paths if fpath in file_to_hash)
        total_files += total
        total_duplicates += duplicates
        if duplicates > 0:
            dir_stats[dir_path] = (duplicates, total)

    # Build hash to directories map for verbose mode
    hash_to_dirs: dict[str, set[Path]] = defaultdict(set)
    for hash_val, paths in file_hashes.items():
        if hash_val in duplicate_hashes:
            for path in paths:
                hash_to_dirs[hash_val].add(path.parent)

    return dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files, cached_hashes


def calculate_redundancy_score(duplicates: int, total: int) -> float:
    """Calculate redundancy score as a ratio of duplicates to total files."""
    return duplicates / total if total > 0 else 0.0


def main():
    parser = argparse.ArgumentParser(
        prog='dedupdir',
        description='Find directories containing duplicate files and report redundancy scores.',
        epilog='The redundancy score is: (duplicate files in dir) / (total files in dir)'
    )
    parser.add_argument(
        'directories',
        nargs='*',
        default=['.'],
        metavar='directory',
        help='Directories to scan (default: current directory)'
    )
    parser.add_argument(
        '-a', '--algorithm',
        default='blake2b',
        choices=['md5', 'sha1', 'sha256', 'blake2b', 'blake2s'],
        help='Hash algorithm to use (default: blake2b)'
    )
    parser.add_argument(
        '-j', '--jobs',
        type=int,
        default=4,
        metavar='N',
        help='Number of parallel hashing jobs (default: 4, use 1 to disable)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Show related directories for each result'
    )
    parser.add_argument(
        '-q', '--quiet',
        action='store_true',
        help='Suppress progress messages'
    )

    args = parser.parse_args()

    # Handle both single directory (backward compat) and multiple directories
    roots = []
    for directory in args.directories:
        root = Path(directory).resolve()
        if not root.exists():
            print(f"Error: '{directory}' does not exist", file=sys.stderr)
            sys.exit(1)
        if not root.is_dir():
            print(f"Error: '{directory}' is not a directory", file=sys.stderr)
            sys.exit(1)
        roots.append(root)

    if not args.quiet:
        if len(roots) == 1:
            print(f"Scanning {roots[0]}...", file=sys.stderr)
        else:
            print(f"Scanning {len(roots)} directories...", file=sys.stderr)
            for root in roots:
                print(f"  {root}", file=sys.stderr)

    # Scan all directories together (find_duplicates handles multiple roots)
    dir_stats, total_duplicates, total_files, file_to_hash, hash_to_dirs, dir_all_files, _ = find_duplicates(roots, algorithm=args.algorithm, jobs=args.jobs, quiet=args.quiet)

    # Print summary before results
    if not args.quiet:
        if not dir_stats:
            print("No duplicate files found.", file=sys.stderr)
        else:
            overall_redundancy = calculate_redundancy_score(total_duplicates, total_files)
            print(f"  Found {total_duplicates} duplicate files in {len(dir_stats)} directories (overall redundancy: {overall_redundancy:.2%})", file=sys.stderr)
            print("", file=sys.stderr)

    if not dir_stats:
        sys.exit(0)

    # Sort by redundancy score (descending), then by path for stable ordering
    sorted_dirs = sorted(
        dir_stats.items(),
        key=lambda x: (-calculate_redundancy_score(x[1][0], x[1][1]), str(x[0]))
    )

    # Print results with relative paths in aligned columns
    # First pass: prepare all rows and find column widths
    rows = []
    max_count_width = 0

    for dir_path, (duplicates, total) in sorted_dirs:
        score = calculate_redundancy_score(duplicates, total)
        # Convert to relative path from one of the roots
        display_path = None
        for root in roots:
            try:
                relative_path = dir_path.relative_to(root)
                display_path = str(root.name / relative_path) if str(relative_path) != '.' else str(root.name)
                break
            except ValueError:
                continue
        if display_path is None:
            # If path is not relative to any root, use absolute
            display_path = dir_path

        count_str = f"{duplicates}/{total}"
        max_count_width = max(max_count_width, len(count_str))
        rows.append((score, count_str, display_path))

    # Second pass: print with aligned columns
    for score, count_str, display_path in rows:
        # Format percentage with proper alignment (right-align to 7 chars including %)
        percent_str = f"{score:7.2%}"
        print(f"{percent_str}  {count_str:>{max_count_width}}  {display_path}")

        # Verbose mode: show related directories
        if args.verbose:
            # Find the actual directory path from display_path
            dir_path = None
            for dp, _ in sorted_dirs:
                try:
                    rel = dp.relative_to(root)
                    if str(rel if str(rel) != '.' else '.') == str(display_path):
                        dir_path = dp
                        break
                except ValueError:
                    if dp == display_path:
                        dir_path = dp
                        break

            if dir_path:
                # Find all hashes in this directory
                dir_hashes = set()
                for fpath in dir_all_files.get(dir_path, []):
                    if fpath in file_to_hash:
                        dir_hashes.add(file_to_hash[fpath])

                # Find related directories (directories that share at least one hash with this dir)
                related_dirs = set()
                for h in dir_hashes:
                    for d in hash_to_dirs[h]:
                        if d != dir_path:
                            related_dirs.add(d)

                # Calculate hypothetical redundancy for each related directory
                related_info = []
                for related_dir in related_dirs:
                    related_files = dir_all_files.get(related_dir, [])
                    total_related = len(related_files)

                    # Count duplicates in related_dir if dir_path didn't exist
                    hypothetical_duplicates = 0
                    for fpath in related_files:
                        if fpath in file_to_hash:
                            h = file_to_hash[fpath]
                            # Count how many OTHER directories (excluding dir_path and related_dir) have this hash
                            other_dirs = hash_to_dirs[h] - {dir_path, related_dir}
                            # If there are other directories with this hash, it's still a duplicate
                            if len(other_dirs) > 0:
                                hypothetical_duplicates += 1

                    hyp_score = calculate_redundancy_score(hypothetical_duplicates, total_related)

                    # Convert to relative path
                    try:
                        rel_path = related_dir.relative_to(root)
                        rel_display = rel_path if str(rel_path) != '.' else '.'
                    except ValueError:
                        rel_display = related_dir

                    related_info.append((hyp_score, hypothetical_duplicates, total_related, rel_display))

                # Sort by hypothetical redundancy score (descending)
                related_info.sort(key=lambda x: (-x[0], str(x[3])))

                # Print related directories
                for hyp_score, hyp_dup, hyp_total, rel_display in related_info:
                    hyp_percent = f"{hyp_score:7.2%}"
                    hyp_count = f"{hyp_dup}/{hyp_total}"
                    print(f"    {hyp_percent}  {hyp_count:>{max_count_width}}  {rel_display}")


if __name__ == '__main__':
    main()
